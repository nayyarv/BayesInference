{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference Lab 1 \n",
    "\n",
    "Varun Nayyar\n",
    "\n",
    "This is an Ipython notebook (an R equivalent now exists) that allows me to put my notes and code in a single place, before exporting a pdf or webpage I can use to review later on. \n",
    "\n",
    "## R\n",
    "\n",
    "### Why?\n",
    "\n",
    "* If you want to see how a recently developed method that was just published works, you can assured that the researcher's will implement their package in R, so seeing it's real world performance is as simple as installing the relevant package. \n",
    "* Any kind of method you want to use is available out of the box, you don't have to do much research and searching to find good implementations\n",
    "* It's the default programming language used by most researching statisticians and knowledge of R is essential for any kind of research statistics. \n",
    "* It is easy to get started with for basic work.\n",
    "* R is a language you have to know for a stats degree, it's NOT optional!\n",
    "\n",
    "### Why Not?\n",
    "\n",
    "* It is a one trick pony - it's good for stats, and that's it\n",
    "    * Very poorly designed languaged\n",
    "    * Over specialized - general purpose work is very difficult with R\n",
    "* Very poor stability, large >100Mb datasets usually cause R to crash.\n",
    "* Constantly in flux, R code written now is unlikely to work next year.\n",
    "* Very difficult to deal with complexity, Object oriented design very poorly implemented.\n",
    "\n",
    "\n",
    "\n",
    "## Python\n",
    "\n",
    "### Why?\n",
    "Python is a full featured and mature language, as such it\n",
    "\n",
    "* Will work a year later, which is not the case with much R code. \n",
    "* Has very well designed features which makes writing complex code much easier. Even simple code is easier in Python.\n",
    "* Has functionality to do many things like webscraping, webpage hosting, file parsing etc.\n",
    "* Performance wise, Python tends to have extremely efficient algorithms implmented and is usually faster than R.  \n",
    "* Superb package support, for nearly anything, you can just import it.\n",
    "    * Numpy for fast (native C speeds) computations\n",
    "    * Scipy for many useful functions in fields like signal processing, optimization, linalg, stats etc\n",
    "    * Matplotlib (and ggplot, seaborn etc) for really nice visualisation\n",
    "    * Pandas for powerful dataframes\n",
    "    * PyMC for efficient Monte Carlo computations, with performance similar to BUGS\n",
    "    * Rpy - call R code from Python!\n",
    "  \n",
    "### Why Not?\n",
    "\n",
    "* Single threaded - does not scale well with available computing power\n",
    "* Limited adoption by statistical community, and as such no packages available for implementation\n",
    "* More general purpose makes initial learning curve a little steeper for stats use, but long term learning curve is much shallower than R.\n",
    "    \n",
    "\n",
    "##Other Languages\n",
    "\n",
    "Julia has become popular of late due to performance promises, however it is even more bleeding edge than R and has very limited support. It is my opinion ([and others](https://darrenjw.wordpress.com/2013/12/23/scala-as-a-platform-for-statistical-computing-and-data-science/)) that taking a programming language and overspecialising is a bad approach (like R, Matlab etc.), I would much rather take a well designed general language and add numeric capabilities. This has the advantage of a well designed language with powerful data types with powerful numerics, as opposed to a language with powerful numerics and little else.\n",
    "\n",
    "For those interested in Big Data, a functional language like [Scala](https://www.coursera.org/course/progfun), OCaml or Haskell would be worth learning (same language family as Julia). Due to the constraints of the language, far greater optimizations can be made, and this allows the program to scale from 1 computer to many with very little additional code. In contrast, Python and R are unable to use more than a single core for computation (unless written explicitly mutli-threaded which is quite difficult and not recommendend in general)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np #very useful\n",
    "from scipy.stats import gamma\n",
    "\n",
    "shape = 24890 + 1\n",
    "rate = 65+ 0.01\n",
    "level = 0.05\n",
    "\n",
    "gamRV = gamma(a = shape, scale = 1/rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a gamma random variable with useful functions and I don't have to keep specifying it's parameters (which is nice). For full usage features, check out the reference below \n",
    "http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378.13762557414162"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower = gamRV.ppf(0.025) # qgamma equivalent\n",
    "lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387.65062700867895"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper = gamRV.ppf(0.975)\n",
    "upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382.879556991\n"
     ]
    }
   ],
   "source": [
    "mean = gamRV.stats('m') #Theoretical mean, isn't that nice!\n",
    "print mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 383.24493677,  382.28731786,  384.23149628,  382.05395576,\n",
       "        381.55219461,  380.03232194,  382.57698159,  385.0215899 ,\n",
       "        384.31104277,  382.77501859,  382.12532371,  384.6899978 ,\n",
       "        380.40238288,  381.06589623,  383.76700382,  382.73350438,\n",
       "        382.6346199 ,  384.18951446,  386.61929799,  380.60383129])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singleMCSet = gamRV.rvs(size = 500) #rgamma equivalent\n",
    "MCSet = gamRV.rvs(size = (250,500)) #larger draw\n",
    "MCSet[0, :20] # let's look at a bit of one of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note that I've drawn 250 lots of  500 values. This is an implementation problem. Python and R are high level languages  with low level language interfaces. This means simple constructs like loops are extremely slow and repeated calls to any function have high overhead. Rather than call gamRV.rvs 250 times, I call it once with more work and get the results with less work. And then you can use functions like `map` in python or `apply` in R to work on datasets quickly (these are loop like, but much quicker).\n",
    "\n",
    "This is really not a problem when it's 250 repeats of 500 samples, but add a few zeros on to the end, and this becomes a problem. You might instead run into issues where you can't store all the data as it requires too much RAM, in which case the loop is unavoidable. \n",
    "\n",
    "Either way, I'll show both approaches here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 378.34156013  377.85339006  378.29282497  377.92307584  378.36065726\n",
      "  378.24494964  377.96984245  378.15971957  378.44345475  378.35025187]\n",
      "[ 387.90265994  387.87673527  388.16069158  387.17139934  387.97616066\n",
      "  387.38449086  387.61748533  386.81922159  387.70048234  387.18734328]\n"
     ]
    }
   ],
   "source": [
    "#loop approach\n",
    "\n",
    "numMCruns = 250\n",
    "numSamples = 500\n",
    "lowVals = np.zeros(numMCruns) \n",
    "upperVals = np.zeros(numMCruns)\n",
    "# empty vectors to store the data, since I know how much storage I need, it's better to preallocate\n",
    "\n",
    "for i in range(numMCruns):\n",
    "    singleMCSet = gamRV.rvs(size = numSamples)\n",
    "    lowVals[i], upperVals[i] = np.percentile(singleMCSet, [2.5, 97.5]) \n",
    "    #double assignment is nice syntax\n",
    "\n",
    "print lowVals[:10] \n",
    "print upperVals[:10] #Let's have a look at what we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Map/apply approach\n",
    "\n",
    "MC = 250\n",
    "nSamps = 500\n",
    "#Turning this into a function for later use\n",
    "def credInts(numMCruns = 250, numSamples=500):\n",
    "    \"\"\"\n",
    "    Function Docstring:\n",
    "    Numpy functions have automatic mapping, here I apply across the columns (axis=1), \n",
    "    so it treats each row as the dataset to apply the function,\n",
    "    If i had it the other way around, i.e. size = (numSamples, numMCruns), \n",
    "    I would apply across the rows, i.e. axis = 0\n",
    "    \"\"\"\n",
    "    MCSet = gamRV.rvs(size = (numMCruns,numSamples)) # 250 rows, 500 columns \n",
    "    lowVals, upperVals = np.percentile(MCSet,[2.5,97.5], axis = 1)\n",
    "    return lowVals, upperVals\n",
    "\n",
    "lowValsMap, upperValsMap = credInts(MC, nSamps)\n",
    "\n",
    "print lowValsMap[:10]\n",
    "print upperValsMap[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now lets plot\n",
    "from matplotlib import pyplot as plt \n",
    "#using matlab style plotting package\n",
    "%matplotlib inline \n",
    "# for displaying in the notebook\n",
    "\n",
    "# from matplotlib.style import use\n",
    "# use(['ggplot', 'fivethirtyeight']) #nicer graphics than default\n",
    "import seaborn as sns\n",
    "sns.set_palette(\"deep\", desat=.6)\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "a = plt.hist(lowValsMap, normed=True, bins = 15)\n",
    "b = plt.hist(upperValsMap, normed=True, bins = 15)\n",
    "\n",
    "#true values\n",
    "plt.axvline(lower, color = 'r', lw=1.5)\n",
    "plt.axvline(upper, lw=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's use our previous function\n",
    "low5k, upp5k = credInts(numMCruns=250, numSamples=5000)\n",
    "plt.figure(figsize = (14,7))\n",
    "plt.xlim((376, 390))\n",
    "a = plt.hist(low5k, normed=True, bins = 15)\n",
    "b = plt.hist(upp5k, normed=True, bins = 15)\n",
    "\n",
    "#true values\n",
    "plt.axvline(lower, lw = 1.5)\n",
    "plt.axvline(upper, color='r', lw=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lowprec, uppprec = credInts(numMCruns=250, numSamples=70000)\n",
    "\n",
    "print \"Lrange: \", np.ptp(lowprec) #ptp = range finder\n",
    "print \"Urange: \", np.ptp(uppprec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes roughly 70,000 samples or so to reduce the range to 0.15 or so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# c)\n",
    "from scipy.stats import nbinom\n",
    "from numpy.random import poisson\n",
    "\n",
    "#Posterior predictive = p(new data| current data)\n",
    "# This is done by drawing from the posterior and using it's drawn value as the parameters in prediction\n",
    "# This is a Monte Carlo approach to the integration problem\n",
    "\n",
    "param = gamRV.rvs(10000)\n",
    "# print param\n",
    "\n",
    "predProb = poisson(param, len(param))\n",
    "# print predProb, len(predProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbinRV = nbinom(n=shape, p = 1-1/(rate+1))\n",
    "# set it to the requisite values\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "# print randVals[:50]\n",
    "x = np.arange(min(predProb)-10, max(predProb)+10) #get integer values for x axis in same region\n",
    "\n",
    "a = plt.plot(x, nbinRV.pmf(x)) #pmf = probability mass functions\n",
    "a = plt.hist(predProb, normed=True, bins = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1d\n",
    "Obviously the algebraic approaches allow us to work with limited computation power, we get more exact answers more quickly, and as such we don't have to worry about issues such as convergence and correlation of samples.\n",
    "\n",
    "However, if we don't choose convenient priors, we do obtain an intractable expression for the posterior, which means we are very limited if we choose to stay with algebraic analysis only.\n",
    "\n",
    "### Part 2\n",
    "\n",
    "For this part, I'm going to use pandas to read the data. pandas.DataFrame has much of the same functionality as R's tables and has many useful features included. I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dat = pd.read_table(\"Data/tuberculosis.txt\", names = ['a', 'c', 'p', 'mu'], sep=' ')\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns # nice multivariate visualisation library\n",
    "p = sns.PairGrid(dat)\n",
    "p.map_upper(plt.scatter)\n",
    "p.map_diag(plt.hist, bins = 20)\n",
    "p.map_lower(sns.kdeplot, cmap = \"PuBu\") \n",
    "# ^^this line quite slow for large datasets - but gives nice visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def relfitness(c, tau, delta, er, es, p):\n",
    "    num = 1.0/tau + 1.0/(delta+er)\n",
    "    denom = 1.0/tau + 1.0/(delta+es+p)\n",
    "    return (1.0-c)* num/denom\n",
    "\n",
    "#define useful function for this\n",
    "\n",
    "phi = relfitness(dat['c'], tau = 0.52, delta = 0.52, er = 0.202, es = 0.52, p = dat['p'])\n",
    "#note we can combine arrays and scalars in the same calculation\n",
    "a = plt.hist(phi, normed = True, bins = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now let's check P(\\phi)<1\n",
    "numLess1 = np.sum(phi<1) # yep as si\n",
    "numTot = len(phi)\n",
    "\n",
    "percentage = 1.0 * numLess1/numTot\n",
    "print percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#not much evidence eh?\n",
    "#credible Interval again\n",
    "low, upp = np.percentile(dat['p'], [2.5, 97.5])\n",
    "print low, upp, upp-low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postLen = len(phi)\n",
    "numIntervals = 0.05 * postLen\n",
    "p = np.sort(dat['p'])\n",
    "#I'm going to do this 250 times, sorting once and then looking up is easier than constantly \n",
    "# recalculating the percentiles!\n",
    "intervalRange = np.zeros(numIntervals)\n",
    "\n",
    "# print postLen, numIntervals\n",
    "# loopmethod\n",
    "for i in range(int(numIntervals)):\n",
    "    intervalRange[i] = p[postLen-numIntervals + i] - p[i] #0 indexing\n",
    "\n",
    "intervalRangeMap = p[postLen-numIntervals:] - p[:numIntervals] \n",
    "#mapping - avoiding loops and using default +- operators.\n",
    "plt.plot(intervalRange, 'o')\n",
    "plt.xlabel(\"Start Index for credible interval\")\n",
    "plt.ylable(\n",
    "\n",
    "index = np.argmin(intervalRangeMap)\n",
    "print intervalRangeMap[index], index\n",
    "print \"Interval is :[{}, {}]\".format(1.0 * index/postLen, 1.0 * (postLen-numIntervals+index)/postLen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Obviously the more symmetric it is, the likelier it is that the central credible interval is the shortest one. This is left skewed, and clearly makes sense that the shortest interval is on the left.\n",
    "\n",
    "##Buffon's Needle\n",
    "\n",
    "Firstly I set the lines for Buffon's needle to be at integer values from [0,d]. This means I can simulate the position of the needle by choosing a random uniform value for the position of the needle centre and then choosing a uniform random angle of the needle (0 to \\pi/2). This allows me calculate it's upper and lower endpoints, and then check if it crosses any integer values, i.e. does it start at value 1 and end in value 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def numhits(ystart, yend):\n",
    "    \"\"\"\n",
    "    Function to calculate the number of needles cross a line, i.e. have changing integer values\n",
    "    Note this is a \"mapped\" function with no loop.\n",
    "    \"\"\"\n",
    "    return np.sum(np.abs(np.floor(yend) - np.floor(ystart)))\n",
    "\n",
    "\n",
    "def Buffon(ratio = 1, d = 20, numtrials = 1e7):\n",
    "    \"\"\"\n",
    "    ratio is length/distance\n",
    "    D = number of lines - this won't change things much\n",
    "    numtrials = how many times do you want to run it, 10^7 is about the max before memory limits come along.\n",
    "    1 billion numbers ~ 80 MB of RAM\n",
    "    \"\"\"\n",
    "\n",
    "    # Any larger trials and we hit memory limitations, and it'd be better to do everything in a loop\n",
    "    length = ratio/2.0\n",
    "    centers = np.random.uniform(0, d, numtrials)\n",
    "    angle = np.random.uniform(0,np.pi, numtrials)\n",
    "\n",
    "    # A = centers + length *\n",
    "\n",
    "    ystart = centers - length* np.sin(angle)\n",
    "    yend = centers + length* np.sin(angle)\n",
    "\n",
    "    crosses = numhits(ystart, yend)\n",
    "\n",
    "    estPi = (2.0/crosses) * numtrials\n",
    "    return estPi\n",
    "    \n",
    "print Buffon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) is quite trivial, it is obvious that at p=1, we have our minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = np.linspace(0.05,1,25) # get some values of p\n",
    "sdList =[]\n",
    "for ratio in p:\n",
    "    sds = [Buffon(ratio, numtrials = 1000) for i in range(1000)] #list comprehension\n",
    "    sdList.append(np.std(sds))\n",
    "\n",
    "# print sdList "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(p, sdList)\n",
    "plt.ylabel(\"Standard Deviation\")\n",
    "plt.xlabel(\"ratio\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the standard deviation is indeed smaller for larger values of p, just as theoretically calculated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
